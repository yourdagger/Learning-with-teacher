{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка-данных\" data-toc-modified-id=\"Подготовка-данных-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка данных</a></span></li><li><span><a href=\"#Исследование-задачи\" data-toc-modified-id=\"Исследование-задачи-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Исследование задачи</a></span></li><li><span><a href=\"#Борьба-с-дисбалансом\" data-toc-modified-id=\"Борьба-с-дисбалансом-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Борьба с дисбалансом</a></span><ul class=\"toc-item\"><li><span><a href=\"#В-качестве-эксперимента-попробуем-не-писать-самостоятельно-функцию,-а-воспользоваться-аргументом-class_weight-при-вызове-логистической-регрессии-и-случайного-леса.-В-аргумент-запишем,-что-у-отрицательных-объектов-будет-вес-1,-а-у-положительных-4\" data-toc-modified-id=\"В-качестве-эксперимента-попробуем-не-писать-самостоятельно-функцию,-а-воспользоваться-аргументом-class_weight-при-вызове-логистической-регрессии-и-случайного-леса.-В-аргумент-запишем,-что-у-отрицательных-объектов-будет-вес-1,-а-у-положительных-4-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>В качестве эксперимента попробуем не писать самостоятельно функцию, а воспользоваться аргументом class_weight при вызове логистической регрессии и случайного леса. В аргумент запишем, что у отрицательных объектов будет вес 1, а у положительных 4</a></span></li><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Вывод</a></span></li></ul></li><li><span><a href=\"#Тестирование-модели\" data-toc-modified-id=\"Тестирование-модели-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Тестирование модели</a></span></li><li><span><a href=\"#Чек-лист-готовности-проекта\" data-toc-modified-id=\"Чек-лист-готовности-проекта-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Чек-лист готовности проекта</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отток клиентов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из «Бета-Банка» стали уходить клиенты. Каждый месяц. Немного, но заметно. Банковские маркетологи посчитали: сохранять текущих клиентов дешевле, чем привлекать новых.\n",
    "\n",
    "Нужно спрогнозировать, уйдёт клиент из банка в ближайшее время или нет. Вам предоставлены исторические данные о поведении клиентов и расторжении договоров с банком. \n",
    "\n",
    "Постройте модель с предельно большим значением *F1*-меры. Чтобы сдать проект успешно, нужно довести метрику до 0.59. Проверьте *F1*-меру на тестовой выборке самостоятельно.\n",
    "\n",
    "Дополнительно измеряйте *AUC-ROC*, сравнивайте её значение с *F1*-мерой.\n",
    "\n",
    "Источник данных: [https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling](https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    data = pd.read_csv('/datasets/Churn.csv')\n",
    "except:\n",
    "\n",
    "    data = pd.read_csv('Churn.csv')\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим типы и заполненность данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что есть пропуски в поле Tenure. Заменим их медианным значением для датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Tenure'].isna(),'Tenure'] = data[\"Tenure\"].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Переведем имена колонок с camelcase на snakecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def change_case(str):\n",
    "#если элемент списка заглавная буква, меняем ее на строчную и добавляем нижнее подчеркивание перед ней\n",
    "#по окончании цикла убираем нижнее подчеркивание в начале строки\n",
    "    return ''.join(['_'+i.lower() if i.isupper()\n",
    "               else i for i in str]).lstrip('_')\n",
    "\n",
    "data.columns = data.columns.to_series().apply(change_case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим есть ли в данных дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучим данные в столбцах с категориальными значениями\n",
    "\n",
    "Посмотрим есть ли в полях неявные дубликаты.\n",
    "\n",
    "Фамилия клиента нас интересовать не будет, поэтому проверим поля geography и gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Female' 'Male']\n",
      "['France' 'Spain' 'Germany']\n"
     ]
    }
   ],
   "source": [
    "print(data['gender'].unique())\n",
    "print(data['geography'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569094e+07</td>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>4.634300</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>7.193619e+04</td>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.989725</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.556570e+07</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.75000</td>\n",
       "      <td>1.562853e+07</td>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569074e+07</td>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.25000</td>\n",
       "      <td>1.575323e+07</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.581569e+07</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        row_number   customer_id  credit_score           age        tenure  \\\n",
       "count  10000.00000  1.000000e+04  10000.000000  10000.000000  10000.000000   \n",
       "mean    5000.50000  1.569094e+07    650.528800     38.921800      4.634300   \n",
       "std     2886.89568  7.193619e+04     96.653299     10.487806      2.989725   \n",
       "min        1.00000  1.556570e+07    350.000000     18.000000      0.000000   \n",
       "25%     2500.75000  1.562853e+07    584.000000     32.000000      2.000000   \n",
       "50%     5000.50000  1.569074e+07    652.000000     37.000000      4.000000   \n",
       "75%     7500.25000  1.575323e+07    718.000000     44.000000      7.000000   \n",
       "max    10000.00000  1.581569e+07    850.000000     92.000000     10.000000   \n",
       "\n",
       "             balance  num_of_products  has_cr_card  is_active_member  \\\n",
       "count   10000.000000     10000.000000  10000.00000      10000.000000   \n",
       "mean    76485.889288         1.530200      0.70550          0.515100   \n",
       "std     62397.405202         0.581654      0.45584          0.499797   \n",
       "min         0.000000         1.000000      0.00000          0.000000   \n",
       "25%         0.000000         1.000000      0.00000          0.000000   \n",
       "50%     97198.540000         1.000000      1.00000          1.000000   \n",
       "75%    127644.240000         2.000000      1.00000          1.000000   \n",
       "max    250898.090000         4.000000      1.00000          1.000000   \n",
       "\n",
       "       estimated_salary        exited  \n",
       "count      10000.000000  10000.000000  \n",
       "mean      100090.239881      0.203700  \n",
       "std        57510.492818      0.402769  \n",
       "min           11.580000      0.000000  \n",
       "25%        51002.110000      0.000000  \n",
       "50%       100193.915000      0.000000  \n",
       "75%       149388.247500      0.000000  \n",
       "max       199992.480000      1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минимальные и максимальные значения в полях age, tenure, balance, num_of_products и estimated_salary не вызывают вопросов\n",
    "\n",
    "Подготовим данные к обучению. Поля row_number, customer_id и surname не помогут нам предсказать уход клиента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['row_number', 'customer_id', 'surname'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit_score        0\n",
       "geography           0\n",
       "gender              0\n",
       "age                 0\n",
       "tenure              0\n",
       "balance             0\n",
       "num_of_products     0\n",
       "has_cr_card         0\n",
       "is_active_member    0\n",
       "estimated_salary    0\n",
       "exited              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#КОД РЕВЬЮЕРА\n",
    "\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ohe = pd.get_dummies(data, columns = ['geography', 'gender'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим датасет на features и target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_unbalanced = data.drop(['exited'], axis = 1)\n",
    "target_unbalanced = data['exited']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследование задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим дисбаланс классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.7963\n",
       "1    0.2037\n",
       "Name: exited, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_unbalanced.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что отрицательных объектов гораздо больше, чем положительных. Но для начала попробуем обучить модель без учёта дисбаланса.\n",
    "\n",
    "Разделим датасет на обучающую, валидационную и тестовую выборки, дважды применив функцию train_test_split. \n",
    "\n",
    "В train_test_split также применим аргумент stratify для одинакового распределения таргетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_unbalanced.copy()\n",
    "target = target_unbalanced.copy()\n",
    "\n",
    "features_train, features_valid_test, target_train, target_valid_test = train_test_split(\n",
    "    features, target, test_size=0.4, random_state=12345, stratify = target)\n",
    "\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(\n",
    "    features_valid_test, target_valid_test, test_size=0.5, random_state=12345, stratify = target_valid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодируем категориальные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['gender', 'geography']),\n",
    "    remainder='passthrough')\n",
    "def one_hot(df):\n",
    "    transformed = transformer.fit_transform(df)\n",
    "    transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names())\n",
    "    # удаляем по 1 из dummy-признаков\n",
    "    transformed_df.drop(['onehotencoder__x0_Male', 'onehotencoder__x1_France'], axis = 1, inplace = True)\n",
    "    return transformed_df\n",
    "\n",
    "features_train = one_hot(features_train)\n",
    "features_valid = one_hot(features_valid)\n",
    "features_test = one_hot(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем обучать модель, масштабируем признаки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train)\n",
    "\n",
    "features_train = scaler.transform(features_train)\n",
    "features_valid = scaler.transform(features_valid) \n",
    "features_test = scaler.transform(features_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель логистической регрессии и сохраним ее в переменную model_unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=12345, C = 2, solver='lbfgs')\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "model_unbalanced_regr = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим значение f1-score на валидационной выборке. В этом исследовании мы хотим добиться максимального значения f1, поскольку эта метрика отражает одновременно полноту и точность модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3107861060329068"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_valid = model.predict(features_valid)\n",
    "f1_score(target_valid, predicted_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение f1 ниже, чем нам бы того хотелось. \n",
    "\n",
    "Посчитаем так же значение auc_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7875760542910633"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(target_valid, probabilities_valid[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение выше, чем у случайной модели (0.5), но достаточно низкое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем вероятность классов с помощью функции predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим, чему равен f1, для различных значений порога классификации в интервале от 0 до 0.6 с шагом 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Порог = 0.00 | f1 = 0.339\n",
      "Порог = 0.02 | f1 = 0.340\n",
      "Порог = 0.04 | f1 = 0.356\n",
      "Порог = 0.06 | f1 = 0.379\n",
      "Порог = 0.08 | f1 = 0.400\n",
      "Порог = 0.10 | f1 = 0.426\n",
      "Порог = 0.12 | f1 = 0.445\n",
      "Порог = 0.14 | f1 = 0.463\n",
      "Порог = 0.16 | f1 = 0.495\n",
      "Порог = 0.18 | f1 = 0.502\n",
      "Порог = 0.20 | f1 = 0.508\n",
      "Порог = 0.22 | f1 = 0.523\n",
      "Порог = 0.24 | f1 = 0.524\n",
      "Порог = 0.26 | f1 = 0.521\n",
      "Порог = 0.28 | f1 = 0.523\n",
      "Порог = 0.30 | f1 = 0.505\n",
      "Порог = 0.32 | f1 = 0.486\n",
      "Порог = 0.34 | f1 = 0.468\n",
      "Порог = 0.36 | f1 = 0.446\n",
      "Порог = 0.38 | f1 = 0.443\n",
      "Порог = 0.40 | f1 = 0.429\n",
      "Порог = 0.42 | f1 = 0.402\n",
      "Порог = 0.44 | f1 = 0.357\n",
      "Порог = 0.46 | f1 = 0.340\n",
      "Порог = 0.48 | f1 = 0.328\n",
      "Порог = 0.50 | f1 = 0.311\n",
      "Порог = 0.52 | f1 = 0.300\n",
      "Порог = 0.54 | f1 = 0.293\n",
      "Порог = 0.56 | f1 = 0.248\n",
      "Порог = 0.58 | f1 = 0.225\n"
     ]
    }
   ],
   "source": [
    "for threshold in np.arange(0, 0.6, 0.02):\n",
    "    predicted_valid = probabilities_one_valid > threshold \n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "\n",
    "    print(\"Порог = {:.2f} | f1 = {:.3f}\".format(\n",
    "        threshold, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наибольшее значение f1 получаем при пороге равном 0.24. Значение метрики уже значительно выше, но все еще недостаточно, чтобы считать нашу модель точной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим какие значения f1 даст нам модель random forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.5946745562130178\n",
      "auc-roc:  0.8603934193023943\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predicted_valid = model.predict(features_valid)\n",
    "f1_score(target_valid, predicted_valid)\n",
    "\n",
    "print('f1: ', f1_score(target_valid, predicted_valid))\n",
    "print('auc-roc: ', roc_auc_score(target_valid, model.predict_proba(features_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем значения f1 и auc_roc выше, чем у линейной регрессии. Теперь поэкспериментируем с гиперпаоаметрами - количеством деревьев и их глубиной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 наилучшей модели на валидационной выборке: 0.613095238095238\n",
      "Кол-во деревьев наилучшей модели на валидационной выборке: 100\n",
      "Глубина наилучшей модели на валидационной выборке: 16\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "best_result = 0\n",
    "for est in range(10, 101, 10):\n",
    "    for depth in range(1,21):\n",
    "        model = RandomForestClassifier(random_state=12345, n_estimators= est, max_depth = depth)\n",
    "        model.fit(features_train, target_train)\n",
    "        predictions_valid = model.predict(features_valid)\n",
    "        result = f1_score(target_valid, predictions_valid)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_prediction = predictions_valid\n",
    "            best_est = est\n",
    "            best_model = model\n",
    "            best_depth = depth\n",
    "model_unbalanced_forrest = best_model\n",
    "\n",
    "print(\"f1 наилучшей модели на валидационной выборке:\", best_result)\n",
    "print(\"Кол-во деревьев наилучшей модели на валидационной выборке:\", best_est) \n",
    "print(\"Глубина наилучшей модели на валидационной выборке:\", best_depth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выбранной модели посчитаем так же auc-roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8642053712188393"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(target_valid, best_model.predict_proba(features_valid)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Для несбалансированной выборки модель случайного леса показывает результаты (f1 и auc) заметно лучше, чем линейная регрессия.\n",
    "\n",
    "Попробуем улучшить их, устранив дисбаланс классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Борьба с дисбалансом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы устранить дисбаланс, воспользуемся методом upsampling. Ранее мы уже выяснили, что положительные и отрицательные объекты соотносятся приблизительно как 1 к 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(features, target, repeat):\n",
    "#создадим переменные для разных значений классов \n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "#уравняем количество положительных и отрицательных объектов в выборке\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "#перемешаем выборку    \n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_train, features_valid_test, target_train, target_valid_test = train_test_split(\n",
    "    features_unbalanced, target_unbalanced, test_size=0.4, random_state=12345, stratify = target)\n",
    "\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(\n",
    "    features_valid_test, target_valid_test, test_size=0.5, random_state=12345, stratify = target_valid_test)\n",
    "\n",
    "#применим функцию к переменным features и target, затем разделим выборку\n",
    "features_train, target_train = upsample(features_train, target_train, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, какой получился баланс для обучающей выборки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.50569\n",
       "0    0.49431\n",
       "Name: exited, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее проделаем те же шаги, что и в исследовании до устранения дисбаланса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['gender', 'geography']),\n",
    "    remainder='passthrough')\n",
    "def one_hot(df):\n",
    "    transformed = transformer.fit_transform(df)\n",
    "    transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names())\n",
    "    # удаляем по 1 из dummy-признаков\n",
    "    transformed_df.drop(['onehotencoder__x0_Male', 'onehotencoder__x1_France'], axis = 1, inplace = True)\n",
    "    return transformed_df\n",
    "\n",
    "features_train = one_hot(features_train)\n",
    "features_valid = one_hot(features_valid)\n",
    "features_test = one_hot(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train)\n",
    "\n",
    "features_train = scaler.transform(features_train)\n",
    "features_valid = scaler.transform(features_valid) \n",
    "features_test = scaler.transform(features_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.5246753246753246\n",
      "auc-roc:  0.7937851019804907\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345, C = 2, solver='lbfgs')\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predicted_valid = model.predict(features_valid)\n",
    "print('f1: ', f1_score(target_valid, predicted_valid))\n",
    "print('auc-roc: ', roc_auc_score(target_valid, model.predict_proba(features_valid)[:,1]))\n",
    "#обученную модель сохраним в переменную upsampled_model\n",
    "upsampled_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.6371681415929203\n",
      "auc-roc:  0.8787996354320624\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predicted_valid = model.predict(features_valid)\n",
    "print('f1: ', f1_score(target_valid, predicted_valid))\n",
    "print('auc-roc: ', roc_auc_score(target_valid, model.predict_proba(features_valid)[:,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1-score стал выше. \n",
    "\n",
    "Теперь поисследуем, каким будет f1 для модели случайного леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.6179921773142113\n",
      "auc-roc:  0.8589793329392058\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predicted_valid = model.predict(features_valid)\n",
    "f1_score(target_valid, predicted_valid)\n",
    "\n",
    "\n",
    "print('f1: ', f1_score(target_valid, predicted_valid))\n",
    "print('auc-roc: ', roc_auc_score(target_valid, model.predict_proba(features_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 и auc-roc изменились незначительно по сравнению с несбалансированной выборкой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На несбалансированной выборке подбор гиперпараметров помог нам улучшить результат.Повторим эту процедуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 наилучшей модели на валидационной выборке: 0.6492622020431328\n",
      "Кол-во деревьев наилучшей модели на валидационной выборке: 75\n",
      "Глубина наилучшей модели на валидационной выборке: 11\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "best_result = 0\n",
    "for est in range(50, 101, 5):\n",
    "    for depth in range(5,20):\n",
    "        model = RandomForestClassifier(random_state=12345, n_estimators= est, max_depth = depth)\n",
    "        model.fit(features_train, target_train)\n",
    "        predictions_valid = model.predict(features_valid)\n",
    "        result = f1_score(target_valid, predictions_valid)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_prediction = predictions_valid\n",
    "            best_est = est\n",
    "            best_model = model\n",
    "            best_depth = depth\n",
    "\n",
    "print(\"f1 наилучшей модели на валидационной выборке:\", best_result)\n",
    "print(\"Кол-во деревьев наилучшей модели на валидационной выборке:\", best_est) \n",
    "print(\"Глубина наилучшей модели на валидационной выборке:\", best_depth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 оказался несколько выше, чем с дефолтными параметрами\n",
    "Сохраним обученную модель в переменную model_upsampled_forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_upsampled_forrest = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Теперь проверим, каков будет результат, если применить технику downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "    \n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345)\n",
    "    return features_downsampled, target_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid_test, target_train, target_valid_test = train_test_split(\n",
    "    features_unbalanced, target_unbalanced, test_size=0.4, random_state=12345, stratify = target)\n",
    "\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(\n",
    "    features_valid_test, target_valid_test, test_size=0.5, random_state=12345, stratify = target_valid_test)\n",
    "\n",
    "#применим функцию к переменным features и target, затем разделим выборку\n",
    "features_train, target_train = downsample(features_train, target_train, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, какой получился баланс для обучающей выборки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.505795\n",
       "0    0.494205\n",
       "Name: exited, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее проделаем те же шаги, что и ранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['gender', 'geography']),\n",
    "    remainder='passthrough')\n",
    "def one_hot(df):\n",
    "    transformed = transformer.fit_transform(df)\n",
    "    transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names())\n",
    "    # удаляем по 1 из dummy-признаков\n",
    "    transformed_df.drop(['onehotencoder__x0_Male', 'onehotencoder__x1_France'], axis = 1, inplace = True)\n",
    "    return transformed_df\n",
    "\n",
    "features_train = one_hot(features_train)\n",
    "features_valid = one_hot(features_valid)\n",
    "features_test = one_hot(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train)\n",
    "\n",
    "features_train = scaler.transform(features_train)\n",
    "features_valid = scaler.transform(features_valid) \n",
    "features_test = scaler.transform(features_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.5275862068965517\n",
      "auc-roc:  0.7925796260715341\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345, solver='lbfgs')\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predicted_valid = model.predict(features_valid)\n",
    "print('f1: ', f1_score(target_valid, predicted_valid))\n",
    "print('auc-roc: ', roc_auc_score(target_valid, model.predict_proba(features_valid)[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат близок тому, что был получен при методе upsample и ниже того, которого мы хотим добиться"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим модель случаного леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.6083650190114068\n",
      "auc-roc:  0.8581140999113214\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predicted_valid = model.predict(features_valid)\n",
    "f1_score(target_valid, predicted_valid)\n",
    "\n",
    "\n",
    "print('f1: ', f1_score(target_valid, predicted_valid))\n",
    "print('auc-roc: ', roc_auc_score(target_valid, model.predict_proba(features_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 наилучшей модели на валидационной выборке: 0.6317829457364341\n",
      "Кол-во деревьев наилучшей модели на валидационной выборке: 60\n",
      "Глубина наилучшей модели на валидационной выборке: 9\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "best_result = 0\n",
    "for est in range(50, 101, 10):\n",
    "    for depth in range(1,31):\n",
    "        model = RandomForestClassifier(random_state=12345, n_estimators= est, max_depth = depth)\n",
    "        model.fit(features_train, target_train)\n",
    "        predictions_valid = model.predict(features_valid)\n",
    "        result = f1_score(target_valid, predictions_valid)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_prediction = predictions_valid\n",
    "            best_est = est\n",
    "            best_model = model\n",
    "            best_depth = depth\n",
    "\n",
    "print(\"f1 наилучшей модели на валидационной выборке:\", best_result)\n",
    "print(\"Кол-во деревьев наилучшей модели на валидационной выборке:\", best_est) \n",
    "print(\"Глубина наилучшей модели на валидационной выборке:\", best_depth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики качества выше, чем те, что были на несбалансированной выборке. Но немного ниже чем результат для upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В качестве эксперимента попробуем не писать самостоятельно функцию, а воспользоваться аргументом class_weight при вызове логистической регрессии и случайного леса. В аргумент запишем, что у отрицательных объектов будет вес 1, а у положительных 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid_test, target_train, target_valid_test = train_test_split(\n",
    "    features_unbalanced, target_unbalanced, test_size=0.4, random_state=12345, stratify = target)\n",
    "\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(\n",
    "    features_valid_test, target_valid_test, test_size=0.5, random_state=12345, stratify = target_valid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['gender', 'geography']),\n",
    "    remainder='passthrough')\n",
    "def one_hot(df):\n",
    "    transformed = transformer.fit_transform(df)\n",
    "    transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names())\n",
    "    # удаляем по 1 из dummy-признаков\n",
    "    transformed_df.drop(['onehotencoder__x0_Male', 'onehotencoder__x1_France'], axis = 1, inplace = True)\n",
    "    return transformed_df\n",
    "\n",
    "features_train = one_hot(features_train)\n",
    "features_valid = one_hot(features_valid)\n",
    "features_test = one_hot(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train)\n",
    "\n",
    "features_train = scaler.transform(features_train)\n",
    "features_valid = scaler.transform(features_valid) \n",
    "features_test = scaler.transform(features_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.5246753246753246\n",
      "auc-roc:  0.7937851019804907\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345, solver='lbfgs', class_weight = {0:1, 1:4})\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predicted_valid = model.predict(features_valid)\n",
    "print('f1: ', f1_score(target_valid, predicted_valid))\n",
    "print('auc-roc: ', roc_auc_score(target_valid, model.predict_proba(features_valid)[:,1]))\n",
    "#обученную модель сохраним в переменную upsampled_model\n",
    "upsampled_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат близкий к upsampling и downsampling, но немного ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.5749235474006116\n",
      "auc-roc:  0.8615211474036851\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345, class_weight = {0:1, 1:4})\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predicted_valid = model.predict(features_valid)\n",
    "f1_score(target_valid, predicted_valid)\n",
    "\n",
    "\n",
    "print('f1: ', f1_score(target_valid, predicted_valid))\n",
    "print('auc-roc: ', roc_auc_score(target_valid, model.predict_proba(features_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также как и для логистической регрессии результат близок, но несколько ниже чем для upsampling и downsampling, которые мы произвели вручную "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  После того, как мы устранили дисбаланс классов, значение метрики f1 значительно увеличилось как для логистической регресси, так и для случайного леса. А значит количество ложно отрицательных и ложно положительных предсказаний стало меньше. При этом метод увеличения выборки дал результаты лучше, чем метод уменьшения, а модель случайного леса показала себя лучше чем логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем модель случайного леса, обученную на увеличенной выборке. Ее сохранил в переменную model_upsampled_forrest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим на тестовой выборке значение f1 для модели upsampled_model с порогом классификации равном 0.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5850052798310453"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test = model_upsampled_forrest.predict(features_test)\n",
    "\n",
    "f1_score(target_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитаем также значение auc roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8425621306977239"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(target_test, model.predict_proba(features_test)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты оказались заметно ниже, чем на валидационной выборке, но выше той цели, которую мы себе ставили.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы добились приемлемого показателя метрики f1. Можем ожидать, что с помощью модели бета-банк сохранит многих клиентов и сэкономит деньги на привлечение новых"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1346,
    "start_time": "2022-12-05T03:45:07.543Z"
   },
   {
    "duration": 182,
    "start_time": "2022-12-05T03:45:08.890Z"
   },
   {
    "duration": 49,
    "start_time": "2022-12-05T03:45:50.543Z"
   },
   {
    "duration": 104,
    "start_time": "2022-12-05T03:46:11.886Z"
   },
   {
    "duration": 14,
    "start_time": "2022-12-05T03:46:15.252Z"
   },
   {
    "duration": 15,
    "start_time": "2022-12-05T03:46:16.243Z"
   },
   {
    "duration": 4,
    "start_time": "2022-12-05T03:46:16.698Z"
   },
   {
    "duration": 12,
    "start_time": "2022-12-05T03:46:17.397Z"
   },
   {
    "duration": 35,
    "start_time": "2022-12-05T03:46:17.820Z"
   },
   {
    "duration": 4,
    "start_time": "2022-12-05T03:46:18.995Z"
   },
   {
    "duration": 6,
    "start_time": "2022-12-05T03:46:19.798Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
